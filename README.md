Latest research trends on NLP:

1. LLM Basics<br />
Paper 1: A review on large Language Models: Architectures, applications, taxonomies, open issues and challenges [Link](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10433480)<br />
Paper 2: Large language models in medicine [Link](https://www.nature.com/articles/s41591-023-02448-8)<br /><br /><br />

2. LLM Hallucination<br />
Paper 1: A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions [Link](https://arxiv.org/pdf/2311.05232)<br />
Paper 2: Hallucination is inevitable: An innate limitation of large language models [Link](https://arxiv.org/pdf/2401.11817)<br />
Paper 3: Hallucination of multimodal large language models: A survey [Link](https://arxiv.org/pdf/2404.18930)<br /><br /><br />

3. Hallucination Mitigation<br />
Paper 1: A comprehensive survey of hallucination mitigation techniques in large language models [Link](https://arxiv.org/pdf/2401.01313)<br /><br /><br />

4. LLM Tokenization<br />
Paper 1: Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization [Link](https://arxiv.org/pdf/2405.17067)<br /><br /><br />

5. Learners (zero/few shots)<br />
Paper 1: Learning Transferable Visual Models From Natural Language Supervision [Link](https://arxiv.org/pdf/2103.00020)<br />
Paper 2: Language Models are Few-Shot Learners [Link](https://arxiv.org/pdf/2005.14165)<br />
Paper 3: A survey of zero-shot learning: Settings, methods, and applications [Link] (https://dl.acm.org/doi/abs/10.1145/3293318)<br /><br /><br />

6. Contrastive Learning<br />
Paper 1: A survey on contrastive self-supervised learning [Link](https://www.mdpi.com/2227-7080/9/1/2)<br /><br /><br />
